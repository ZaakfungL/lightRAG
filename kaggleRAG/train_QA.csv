id,question,answer,answer_value,answer_unit,ref_id,ref_url,supporting_materials,explanation
q003,What is the name of the benchmark suite presented in a recent paper for measuring inference energy consumption?,The ML.ENERGY Benchmark,ML.ENERGY Benchmark,is_blank,['chung2025'],['https://arxiv.org/pdf/2505.06371'],"We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments...",Quote
q009,What were the net CO2e emissions from training the GShard-600B model?,4.3 tCO2e,4.3,tCO2e,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],"""Training GShard-600B used 24 MWh and produced 4.3 net tCO 2 e.""",Quote
q054,What is the model size in gigabytes (GB) for the LLaMA-33B model?,64.7 GB,64.7,GB,['chen2024'],['https://arxiv.org/pdf/2405.01814'],Table 3: Large language models used for evaluation.,Table 3
q062,"What was the total electricity consumption of all Google Cloud TPU pods worldwide in 2023, in megawatt-hours?",Unable to answer with confidence based on the provided documents.,is_blank,MWh,is_blank,is_blank,is_blank,is_blank
q075,True or False: Hyperscale data centers in 2020 achieved more than 40% higher efficiency compared to traditional data centers.,TRUE,1,is_blank,"['wu2021b','patterson2021']","['https://arxiv.org/abs/2108.06738','https://arxiv.org/abs/2104.10350']","Wu 2021, body text near Fig. 1: ""…between traditional and highly optimized hyperscale data centers, power usage effectiveness (PUE) has a stark difference - **more than 40% higher efficiency** for hyperscale data centers (Figure 1)."" Patterson 2021 (p.3): ""…In 2020, [US average PUE] was **1.59**… The PUE for the Iowa datacenter… is **1.11**.""","The >40% statement is explicit in Wu. Patterson's PUE numbers (1.59 vs 1.11) provide a numeric example consistent with ""more than 40% higher efficiency."""
q078,"For every medium-length GPT-3 completion (prompt= 800 words; response 150 - 300 words), roughly how many 500 mL bottles of water does the model 'drink'?",0.02 to 0.1 bottles,"[0.02,0.1]",500 mL bottles,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""Additionally, GPT-3 needs to -drink- (i.e., consume) a 500ml bottle of water for roughly 10 - 50 medium-length responses, depending on when and where it is deployed."", ""More specifically, we consider a medium-sized request, each with approximately=800 words of input and 150 - 300 words of output [30]. ""","The paper states that one 500ml bottle is consumed for every 10 to 50 responses. Therefore, one response consumes 1/50 to 1/10 of a bottle, which is 0.02 to 0.1 bottles."
q091,"From a sample of 60 papers from top AI conferences, what is the difference between the percentage of CVPR papers that target accuracy and the percentage of CVPR papers that target efficiency?",55%,55,percent,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],"""A large majority of the papers target accuracy (90% of ACL papers, 80% of NeurIPS papers and 75% of CVPR papers). Moreover, for both empirical AI conferences (ACL and CVPR) only a small portion (10% and 20% respectively) argue for a new efficiency result.""",Requires calculation (75-20)
q102,"True or False: The AI Act makes energy consumption data from providers publicly available to NGOs, analysts, and the general public.",FALSE,0,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],Section 4.3 Transparency: 'Where the Act does mandate disclosure... this information is restricted to authorities and is not accessible to downstream providers... or the general public.',Quote
q105,What is the projected maximum batch size (in samples) for fine-tuning a Mixtral model with a projected GPU capacity of 100?,28 samples per batch,28,samples,['xia2024'],['https://arxiv.org/pdf/2408.04693'],Figure 13,Figure
q106,What was the approximate speedup in inference throughput for LLaMA-7B when using NVIDIA A100 GPUs compared to V100 GPUs?,2x,2,multiplier,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"""anywhere from a 2 times (7B) … increase … on the A100 when compared to the V100""",Quote
q124,"What is the estimated total operational water consumption for training GPT-3 in Microsoft's U.S. data centers, in million liters?",5.439 million liters,5439000,liters,['li2025b'],['https://arxiv.org/pdf/2304.03271'],Table 1,Table
q135,True or False: The authors propose that sustainability impact assessments (SIAs) should apply not just to high-risk AI but across all AI systems.,TRUE,1,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],Section 5.4 Sustainability Impact Assessments: 'these assessments should not be limited to high-risk AI models but should also apply to all AI systems...',Quote
q139,"As of 2023, what was the water use effectiveness (WUE) for AWS data centers, in L/kWh?",0.18 L/kWh,0.18,L/kWh,['amazon2023'],['https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf'],0.18 Liters of water per kilowatt-hour (L/kWh) water use effectiveness (WUE) for AWS data centers,Quote
q146,True or False: Local inference was emphasized as a sustainability measure because it reduces both network overhead and carbon footprint when deploying large language models.,TRUE,1,is_blank,['khan2025'],['https://arxiv.org/pdf/2504.06307'],Section III.B.1: 'local inference allows models to run directly on user devices... significantly reduces both network overhead and carbon footprint',Quote
q153,True or False: Tracking the runtime of a training job is an important step for estimating compute cost in GPU-based or cloud environments.,TRUE,1,is_blank,['strubell2019'],['https://arxiv.org/pdf/1906.02243'],"""Authors should report training time and sensitivity to hyperparameters. Our experiments suggest that it would be beneficial to directly compare different models to perform a cost-benefit (accuracy) analysis. To address this, when proposing a model that is meant
to be re-trained for downstream use, such as retraining on a new domain or fine-tuning on a new task, authors should report training time and computational resources required, as well as model sensitivity to hyperparameters""",Quote
q158,"For the LLaMA-65B model, what was the maximum performance improvement (latency reduction) achieved by enabling the automated resource utilization overlapping feature in the 2025 Chen et al. study?",13.20%,13.2,percent,['chen2024'],['https://arxiv.org/pdf/2405.01814'],"As illustrated in Figure 14, the LLaMA-65B model experiences a significant improvement in performance, achieving up to a 13.2% with through automated resource utilization overlapping.",Quote
q164,How much does an elephant weigh?,Unable to answer with confidence based on the provided documents.,is_blank,lbs,is_blank,is_blank,is_blank,is_blank
q166,"Which of the following five large NLP DNNs has the highest energy consumption: Meena, T5, GPT-3, GShard-600B, or Switch Transformer?",GPT-3,GPT-3,is_blank,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],Figure 3,Figure
q170,How many days of CO₂ emissions from an average American life are equivalent to training BERT base?,14.4,14.4,days,['strubell2019'],['https://arxiv.org/pdf/1906.02243'],Table 1 and Table 3,1438 lbs / 99.8 lbs/day = 14.4 
q200,True or False: The Transformer architecture eventuallly outperforms the Evolved Transformers architecture on the WMT'24 EN-DE BLUE task as the model sizes grow.,FALSE,0,is_blank,['patterson2021'],['https://arxiv.org/pdf/2104.10350'],Figure 4,Figure
q202,"What dataset of 5,842 labeled entries was used to test energy-efficient large language models in the financial domain?",Financial Sentiment Analysis,Financial Sentiment Analysis,is_blank,['khan2025'],['https://arxiv.org/pdf/2504.06307'],"Section IV.B: 'Our dataset, Financial Sentiment Analysis [18], comprises 5,842 entries...'",Quote
q203,True or False: Eight T4 spot instances could be more cost-efficient than a DGX-2 node for distributed training.,TRUE,1,is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],Abstract: 'eight T4 instances... trumping both more centralized and powerful hardware such as DGX-2',Quote
q207,True or False: The 2023 US Executive Order regarding AI mentioned the greenhouse gas emissions or energy usage of AI.,FALSE,0,is_blank,luccioni2025b,['https://arxiv.org/pdf/2504.00797'],"""Similarly, sustainability considerations were also lacking in the 2023 US Executive Order regarding AI [20], which did not mention Al's greenhouse gas emissions nor energy usage...""",Quote
q211,"True or False: Under Germany's 2023 Energy Efficiency Act, data centers must run on 100% renewable energy by January 1, 2027.",TRUE,1,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"Section 3.3 German 2023 Energy Efficiency Act: 'requiring... 50 % renewable energy, increasing that factor to 100% by 1 Jan 2027.'",Quote
q215,"Out of a sample of 60 papers from top AI conferences, how many papers from ACL targeted both accuracy and efficiency?",2 papers,2,papers,['schwartz2019'],['https://arxiv.org/pdf/1907.10597'],Figure 2,Figure
q221,"According to recent estimates, what percentage of a model's total lifecycle energy use can inference account for?",Up to 90%,90,percent,['jegham2025'],['https://arxiv.org/pdf/2505.09598'],"Recent estimates suggest inference can account for up to 90% of a model's total lifecycle energy use [14, 15].",Quote
q230,True or False: The AI Act requires providers to report both training and inference energy consumption for general-purpose AI models.,FALSE,0,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"Section 4.3 Transparency: 'this requirement... only covers the energy used during the model's development phase, but leaves out the inference phase.'",Quote
q231,True or False: The AI Act currently requires providers to report energy use during the inference phase of AI models.,FALSE,0,is_blank,['ebert2024'],['https://arxiv.org/pdf/2410.06681'],"Section 4.3 Transparency: 'this requirement... only covers the energy used during the model's development phase, but leaves out the inference phase.'",Quote
q246,True or False: New AI data centers often rely on air cooling due to high server power densities.,FALSE,0,is_blank,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""In general, new data centers dedicated to AI training often rely on liquid cooling due to the high server power densities.""",Quote
q253,By what factor did platform-level caching improve the power efficiency of the inference workload for the cross-lingual Transformer language model described in Wu et al. (2021)?,6.7x,6.7,multiplier,['wu2021a'],['https://arxiv.org/pdf/2111.00364'],"""Platform-Level Caching. Starting with a CPU server baseline, application-level caching improves power efficiency by 6.7x.""",Quote
q262,What is the estimated CO2 emissions in pounds from training a BERT base model for 79 hours using  64 V100 GPUs?,1438 lbs,1438,lbs,['strubell2019'],['https://arxiv.org/pdf/1906.02243'],Table 3,Table
q263,"According to a recent paper, what percentage of total compute demand does ML inference reportedly account for?",80-90%,"[80,90]",percent,['chung2025'],['https://arxiv.org/pdf/2505.06371'],"This particularly impacts serving real-world services as ML inference reportedly accounts for 80-90% of the total compute demand [10, 27, 50, 51].",Quote
q272,How many U.S. household-years of electricity consumption is training a 6.1B-parameter language model equivalent to?,1.3,1.3,household-years,"['dodge2022','strubell2019']","['https://arxiv.org/pdf/2206.05229','https://arxiv.org/pdf/1906.02243']","Dodge 2022: ""The 6.1B parameter model consumed 13.8 MWh."" Strubell 2019: ""Average U.S. household consumption ≈ 10,715 kWh/yr.""",13.8 MWh ÷ 10.7 MWh/yr ≈ 1.3 household-years.
q278,True or False: Egress costs in geo-distributed NLP experiments could account for more than 90% of the total cost per VM.,TRUE,1,is_blank,['erben2023'],['https://arxiv.org/pdf/2306.03163'],"Figure 11b, 'external egress cost for GC is $4.329/h, more than 90% of the total cost per VM'","Figure, Quote"
q280,"Given the total pre-training GPU hours and the number of GPUs used, estimate the total wall-clock time in days required to pre-train the JetMoE-8B model.",~13 days,13,days,['shen2024'],['https://arxiv.org/pdf/2404.07413'],"""…30,000 H100 GPU hours… We conduct training on a cluster containing … 96 H100s.""","Math: wall_clock_hours ≈ 30,000 GPUh ÷ 96 GPUs = 312.5 h; days ≈ 312.5 ÷ 24 ≈ 13.0 days (pretraining; alignment 60 GPUh is separate)."
q282,"What is the term for the amount of water evaporated, transpired, or incorporated into products, and is defined as 'water withdrawal minus water discharge'?",Water consumption,Water consumption,is_blank,['li2025b'],['https://arxiv.org/pdf/2304.03271'],"""Water consumption: It is defined as -water withdrawal minus water discharge"", and means the amount of water ""evaporated, transpired, incorporated into products or crops, or otherwise removed from the immediate water environment- [13].""",Quote
q296,What was the observed range of inference energy per second for LLaMA-65B across GPU shard configurations?,300 to 1000 W,"[300,1000]",W,['samsi2024'],['https://arxiv.org/pdf/2310.03003'],"""energy per second … is on the order of 300 Watts to 1 Kilowatt … from 8 GPUs to 32 GPUs""",Quote
q297,"When comparing small and large versions of Qwen models in zero-shot classification, how many times more energy did the 72B version consume than the 7B version?",8.720430108,8.720430108,multiplier,['zschache2025'],['https://arxiv.org/pdf/2508.14170'],Table B1: Qwen 2.5 7B - Energy 5.58 Wh; Qwen 2.5 72B - Energy 48.66 Wh, 48.66/5.58 
q304,By what percentage did Qwen's carbon emissions fall when applying quantization and local inference for sentiment classification?,55.6,55.6,percent,['khan2025'],['https://arxiv.org/pdf/2504.06307'],"Table III: Qwen - Before 0.009 kg CO2, After 0.004 kg CO2",0.009→0.004=55.6%
q306,How many widely used model architectures across different tasks were included in the latest iteration of the ML.ENERGY Benchmark?,40,40,models,['chung2025'],['https://arxiv.org/pdf/2505.06371'],"We then highlight results from the latest iteration of the benchmark, including energy measurements of 40 widely used model architectures across 6 different tasks...",Quote
q316,"In the analysis of training a Llama-3.1 scale model, what was the estimated health cost, In USD, when training in Altoona, Iowa?",$2.51 million,2510000,USD,['han2024'],['https://arxiv.org/pdf/2412.06288'],Table 3,Table

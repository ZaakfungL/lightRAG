id,type,title,year,citation,url
amazon2023,report,2023 Amazon Sustainability Report,2023,Amazon Staff. (2023). Amazon Sustainability Report. https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf,https://sustainability.aboutamazon.com/2023-amazon-sustainability-report.pdf
chen2024,paper,Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation,2024,"Shaoyuan Chen, Wencong Xiao, Yutong Lin, Mingxing Zhang, Yingdi Shan, Jinlei Jiang, Kang Chen, Yongwei Wu. (2024). Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation. arXiv. https://arxiv.org/pdf/2405.01814",https://arxiv.org/pdf/2405.01814
chung2025,paper,The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization,2025,"Jae-Won Chung, Jiachen Liu, Jeff J. Ma, Ruofan Wu, Oh Jun Kweon, Yuxuan Xia, Zhiyu Wu, Mosharaf Chowdhury (2025). The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization. arXiv. https://arxiv.org/pdf/2505.06371",https://arxiv.org/pdf/2505.06371
cottier2024,paper,The Rising Costs of Training Frontier AI Models,2024,"Ben Cottier, Robi Rahman, Loredana Fattorini, Nestor Maslej, David Owen. (2024). The Rising Costs of Training Frontier AI Models. arXiv. https://arxiv.org/pdf/2405.21015",https://arxiv.org/pdf/2405.21015
dodge2022,paper,Measuring the Carbon Intensity of AI in Cloud Instances,2022,"Jesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, Will Buchanan (2022). Measuring the Carbon Intensity of AI in Cloud Instances. FAccT '22. https://arxiv.org/pdf/2206.05229",https://arxiv.org/pdf/2206.05229
ebert2024,paper,"AI, Climate, and Regulation: From Data Centers to the AI Act",2024,"Kai Ebert, Nicolas Alder, Ralf Herbrich, & Philipp Hacker (2024). AI, climate, and regulation: From data centers to the AI act arXiv. https://arxiv.org/html/2410.06681v1",https://arxiv.org/pdf/2410.06681
erben2023,paper,How Can We Train Deep Learning Models Across Clouds and Continents? An Experimental Study,2023,"Alexander Erben, Ruben Mayer, & Hans-Arno Jacobsen (2023). How can we train deep learning models across clouds and continents? an experimental study arXiv. https://arxiv.org/pdf/2306.03163",https://arxiv.org/pdf/2306.03163
fernandez2025,paper,Energy Considerations of Large Language Model Inference and Efficiency Optimizations,2025,"Jared Fernandez Clara Na , Vashisth Tiwari , Yonatan Bisk , Sasha Luccioni, Emma Strubell. https://arxiv.org/pdf/2504.17674.pdf",https://arxiv.org/pdf/2504.17674
griggs2024,paper,Mélange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity,2024,"Tyler Griggs, Xiaoxuan Liu, Jiaxiang Yu, Doyoung Kim, Wei-Lin Chiang, Alvin Cheung, Ion Stoica. (2024). MÃ©lange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity. arXiv. https://arxiv.org/pdf/2404.14527",https://arxiv.org/pdf/2404.14527
han2024,paper,The Unpaid Toll: Quantifying the Public Health Impact of AI,2024,"Yuelin Han, Zhifeng Wu, Pengfei Li, Adam Wierman, Shaolei Ren (2024). The Unpaid Toll: Quantifying the Public Health Impact of AI. arXiv. https://arxiv.org/pdf/2412.06288",https://arxiv.org/pdf/2412.06288
jegham2025,paper,"How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",2025,"Nidhal Jegham, Marwan Abdelatti, Lassad Elmoubarki, Abdeltawab Hendawi (2025). How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference. arXiv. https://arxiv.org/pdf/2505.09598",https://arxiv.org/pdf/2505.09598
khan2025,paper,"Optimizing Large Language Models: Metrics, Energy Efficiency, and Case Study Insights",2025,"Tahniat Khan, Soroor Motie, Sedef Akinli Kocak, & Shaina Raza (2025). Optimizing large language models: Metrics, energy efficiency, and case study insights arXiv. https://arxiv.org/pdf/2504.06307",https://arxiv.org/pdf/2504.06307
kim2025,paper,Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading,2025,"Kihyun Kim, Jinwoo Kim, Hyunsun Chung, Myung-Hoon Cha, Hong-Yeon Kim, Youngjae Kim. (2025). Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading. arXiv. https://arxiv.org/pdf/2504.11816",https://arxiv.org/pdf/2504.11816
li2025a,paper,FLM-101B: An Open LLM and How to Train It with $100K Budget,2025,"Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, & Yequan Wang. (2025). FLM-101B: An Open LLM and How to Train It with $100K Budget. arXiv. https://arxiv.org/pdf/2309.03852",https://arxiv.org/pdf/2309.03852
li2025b,paper,Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models,2023,"Pengfei Li, Jianyi Yang, Mohammad A. Islam, Shaolei Renn (2023). Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models. arXiv. https://arxiv.org/pdf/2304.03271",https://arxiv.org/pdf/2304.03271
luccioni2023,paper,Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning,2023,"Alexandra Sasha Luccioni, Alex Hernandez-Garcia (2023). Counting Carbon: A Survey of Factors Influencing the Emissions of Machine Learning. arXiv. https://arxiv.org/pdf/2302.08476",https://arxiv.org/pdf/2302.08476
luccioni2024,paper,Power Hungry Processing: Watts Driving the Cost of AI Deployment?,2024,"Alexandra Sasha Luccioni, Yacine Jernite, Emma Strubell. (2024). Power Hungry Processing: Watts Driving the Cost of AI Deployment?. arXiv. https://arxiv.org/pdf/2311.16863",https://arxiv.org/pdf/2311.16863
luccioni2025a,paper,From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate,2025,"Alexandra Sasha Luccioni, Emma Strubell, Kate Crawford (2025). From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate. FAccT '25. https://arxiv.org/pdf/2501.16548",https://arxiv.org/pdf/2501.16548
luccioni2025b,paper,Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice,2025,"Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefala, Nyalleng Moorosi (2025). Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice. arXiv. https://arxiv.org/pdf/2504.00797",https://arxiv.org/pdf/2504.00797
luccioni2025c,paper,Misinformation by Omission: The Need for More Environmental Transparency in AI,2025,"Alexandra Sasha Luccioni, Boris Gamazaychikov, Theo Alves da Costa, Emma Strubell (2025). Misinformation by Omission: The Need for More Environmental Transparency in AI. arXiv. https://arxiv.org/pdf/2506.15572",https://arxiv.org/pdf/2506.15572
morrison2025,paper,Holistically Evaluating the Environmental Impact of Creating Language Models,2025,"Jacob Morrison, Clara Na, Jared Fernandez, Tim Dettmers, Emma Strubell, Jesse Dodge (2025). Holistically Evaluating the Environmental Impact of Creating Language Models. arXiv. https://arxiv.org/pdf/2503.05804",https://arxiv.org/pdf/2503.05804
patterson2021,paper,Carbon Emissions and Large Neural Network Training,2021,"David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, Jeff Dean (2021). Carbon Emissions and Large Neural Network Training. arXiv. https://arxiv.org/pdf/2104.10350",https://arxiv.org/pdf/2104.10350
rubei2025,paper,Prompt engineering and its implications on the energy consumption of Large Language Models,2025,"Riccardo Rubei, Aicha Moussaid, Claudio Di Sipio, & Davide Di Ruscio (2025). Prompt engineering and its implications on the energy consumption of large language models arXiv. https://arxiv.org/pdf/2501.05899",https://arxiv.org/pdf/2501.05899
samsi2024,paper,From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference,2023,"Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, & Vijay Gadepally (2023). From words to watts: Benchmarking the energy costs of large language model inference arXiv. https://arxiv.org/pdf/2310.03003",https://arxiv.org/pdf/2310.03003
schwartz2019,paper,Green AI,2019,"Roy Schwartz, Jesse Dodge, Noah A. Smith, Oren Etzioni (2019). Green AI. Communications of the ACM. https://arxiv.org/pdf/1907.10597",https://arxiv.org/pdf/1907.10597
shen2024,paper,JetMoE: Reaching Llama2 Performance with 0.1M Dollars,2024,"Yikang Shen, Zhen Guo, Tianle Cai, Zengyi Qin. (2024). JetMoE: Reaching Llama2 Performance with 0.1M Dollars. arXiv. https://arxiv.org/pdf/2404.07413",https://arxiv.org/pdf/2404.07413
stone2022,paper,Artificial Intelligence and Life in 2030: The One Hundred Year Study on Artificial Intelligence,2022,"Peter Stone, Rodney Brooks, Erik Brynjolfsson, Ryan Calo, Oren Etzioni, Greg Hager, Julia Hirschberg, Shivaram Kalyanakrishnan, Ece Kamar, Sarit Kraus, Kevin Leyton-Brown, David Parkes, William Press, AnnaLee Saxenian, Julie Shah, Milind Tambe, Astro Teller (2022). Artificial Intelligence and Life in 2030: The One Hundred Year Study on Artificial Intelligence. arXiv. https://arxiv.org/pdf/2211.06318",https://arxiv.org/pdf/2211.06318
strubell2019,paper,Energy and Policy Considerations for Deep Learning in NLP,2019,"Emma Strubell, Ananya Ganesh, Andrew McCallum (2019). Energy and Policy Considerations for Deep Learning in NLP. ACL. https://arxiv.org/pdf/1906.02243",https://arxiv.org/pdf/1906.02243
wu2021a,paper,"Sustainable AI: Environmental Implications, Challenges and Opportunities",2021,"Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga Behram, James Huang, Charles Bai, Michael Gschwind, Anurag Gupta, Myle Ott, Anastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee, Hsien-Hsin S. Lee, Bugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat, Kim Hazelwood (2021). Sustainable AI: Environmental Implications, Challenges and Opportunities. arXiv. https://arxiv.org/pdf/2111.00364",https://arxiv.org/pdf/2111.00364
wu2021b,paper,SOCIO-TECHNOLOGICAL CHALLENGES AND OPPORTUNITIES: PATHS FORWARD,2021,"Carole-Jean Wu, Srilatha Manne, Parthasarathy Ranganathan, Sarah Bird, & Shane Greenstein (2021). SOCIO-TECHNOLOGICAL CHALLENGES AND OPPORTUNITIES: PATHS FORWARD arXiv. https://arxiv.org/pdf/2108.06738",https://arxiv.org/pdf/2108.06738
xia2024,paper,Understanding the Performance and Estimating the Cost of LLM Fine-Tuning,2024,"Yuchen Xia, Jiho Kim, Yuhan Chen, Haojie Ye, Souvik Kundu, Cong Hao, Nishil Talati. (2024). Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. arXiv. https://arxiv.org/pdf/2408.04693",https://arxiv.org/pdf/2408.04693
zschache2025,paper,Comparing energy consumption and accuracy in text classification inference,2025,"Johannes Zschache, & Tilman Hartwig (2025). Comparing energy consumption and accuracy in text classification inference arXiv. https://arxiv.org/pdf/2508.14170",https://arxiv.org/pdf/2508.14170 
